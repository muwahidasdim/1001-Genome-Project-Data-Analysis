# -*- coding: utf-8 -*-
"""Data Analysis and Predictive Modelling

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17gWjCKpEF5N53Fa5hbty6VMLUW822kz7

# scaffold_analysis_c24.csv

## Data Reading
"""

# prompt: read csv and perform EDA

import pandas as pd

# Read the CSV file into a Pandas DataFrame
df_c24 = pd.read_csv('/content/scaffold_analysis_c24.csv')
df_b0 = pd.read_csv('/content/scaffold_analysis_bur_0.csv')
df_l1 = pd.read_csv('/content/scaffold_analysis_ler_1.csv')
df_k0 = pd.read_csv('/content/scaffold_analysis_kro_0.csv')


# # # Print the first five rows of the DataFrame
# df.head()
df_b0.head()

"""## Preprocessing"""

# prompt: split Scaffold in df on '|' and then split the second part on '_' and then make two new features in which the first part before '_' will be saved and named as "chromosome" and second part after '_' will be saved and named as 'arm'

import pandas as pd
def split(df):
  df['Scaffold'] = df['Scaffold'].str.split('|').str.get(-1)
  df[['chromosome', 'arm']] = df['Scaffold'].str.split('_', n=1, expand=True)
  df.drop(columns=['Scaffold'], inplace=True)

split(df_b0)
split(df_c24)
split(df_l1)
split(df_k0)
# df.head()
df_b0.head()

def mapping(df):
    df['chromosome'] = df['chromosome'].str.strip()

    chromosome_mapping = {
        'Chr1': 0,
        'Chr2': 1,
        'Chr3': 2,
        'Chr4': 3,
        'Chr5': 4,
    }

    df['chromosome'] = df['chromosome'].map(chromosome_mapping)


    arm_mapping = {
        'left_arm': 0,
        'right_arm': 1,
    }

    df['arm'] = df['arm'].map(arm_mapping)

    # print(df.head())


mapping(df_b0)
mapping(df_c24)
mapping(df_l1)
mapping(df_k0)

df_k0.head()

"""## Basic Analysis"""

import seaborn as sns
import matplotlib.pyplot as plt

def co(df):
  # Calculate the correlation matrix
  correlation_matrix = df.corr()

  # Display the correlation matrix using Seaborn
  sns.heatmap(correlation_matrix, annot=True)
  plt.show()

co(df_b0)
co(df_c24)
co(df_l1)
co(df_k0)

# prompt: perform some statistical analysis on the data frames for comparisons of mean of Length and plot the results

import matplotlib.pyplot as plt
# Perform one-way ANOVA to compare the means of Length across the dataframes
import pandas as pd
import numpy as np
from scipy import stats

# Create a list of dataframes
df_list = [df_b0, df_c24, df_l1, df_k0]

# Extract Length column from each dataframe
length_data = [df['T'] for df in df_list]

# Perform one-way ANOVA
f_value, p_value = stats.f_oneway(*length_data)

# Print the results
print("F-value:", f_value)
print("P-value:", p_value)

# If the p-value is less than 0.05, we can reject the null hypothesis that the means are equal
if p_value < 0.05:
    print("There is a statistically significant difference in the means of Length across the dataframes.")

# Plot the means of Length for each dataframe
plt.bar(['b0', 'c24', 'l1', 'k0'], [np.mean(df['T']) for df in df_list])
plt.xlabel('Dataframe')
plt.ylabel('Mean of T')
plt.title('Comparison of T across Dataframes')
plt.show()

"""## Decision Tree Classifier for 'Arm'

"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report


DT_Data=df
DT_Data.drop('chromosome', axis=1)
DT_Data.drop('Length', axis=1)

# Separate features and target
features = DT_Data.drop('arm', axis=1)
target = DT_Data['arm']

# Stratified sampling
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.1, random_state=42)
for train_index, test_index in splitter.split(features, target):
    X_train, X_test = features.loc[train_index], features.loc[test_index]
    y_train, y_test = target.loc[train_index], target.loc[test_index]

# Train the Decision Tree classifier
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Evaluate the model on the test set
print("Accuracy on test set:", model.score(X_test, y_test))

# Calculate the classification report
report = classification_report(y_test, y_pred)

# Print the report
print(report)

"""## Decision Tree Classifier for 'chromosome'

"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.metrics import classification_report

DT_Data=df_b0

DT_Data.drop('arm', axis=1)
DT_Data.drop('Length', axis=1)

# Separate features and target
features = DT_Data.drop('chromosome', axis=1)
target = DT_Data['chromosome']

# Stratified sampling
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in splitter.split(features, target):
    X_train, X_test = features.loc[train_index], features.loc[test_index]
    y_train, y_test = target.loc[train_index], target.loc[test_index]

# Train the Decision Tree classifier
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

# Evaluate the model on the test set
print("Accuracy on test set:", model.score(X_test, y_test))

# Calculate the classification report
report = classification_report(y_test, y_pred)

# Print the report
print(report)
report = classification_report(y_test, y_pred, output_dict=True)
print(report)
# Create a bar plot for each metric
metrics = ['precision', 'recall', 'f1-score']
labels = ['0', '1', '2', '3', '4']

for metric in metrics:
    values = [report[str(label)][metric] for label in labels]
    # labels = report.keys()
    plt.bar(labels, values)
    plt.xlabel('Class')
    plt.ylabel(metric)
    plt.title(f'{metric.capitalize()} by Class')
    plt.show()

"""## SVC"""

from sklearn.svm import SVC

# Separate features and target
# features = df.drop('arm', axis=1)
# target = df['arm']

# Stratified sampling
# splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
# for train_index, test_index in splitter.split(features, target):
#     X_train, X_test = features.loc[train_index], features.loc[test_index]
#     y_train, y_test = target.loc[train_index], target.loc[test_index]

# Train the SVM classifier
model = SVC(random_state=42)
model.fit(features, target)

# Evaluate the model on the test set
print("Accuracy on test set:", model.score(X_test, y_test))

# Calculate the classification report
report = classification_report(y_test, y_pred)

# Print the report
print(report)
# Get the classification report as a dictionary

"""## XGBoost Classifier"""

from sklearn.metrics import classification_report
from xgboost import XGBClassifier
#Tell what xgboost is and its working principle

XGB_Data=df_b0

XGB_Data.drop('arm', axis=1)
XGB_Data.drop('Length', axis=1)

# Separate features and target
features = XGB_Data.drop('chromosome', axis=1)
target = XGB_Data['chromosome']

# Stratified sampling
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in splitter.split(features, target):
    X_train, X_test = features.loc[train_index], features.loc[test_index]
    y_train, y_test = target.loc[train_index], target.loc[test_index]

# Train the XGBoost classifier
model = XGBClassifier(random_state=42)
model.fit(features, target)

# Evaluate the model on the test set
print("Accuracy on test set:", model.score(X_test, y_test))

y_pred = model.predict(X_test)

# Calculate the classification report
report = classification_report(y_test, y_pred)

# Print the report
print(report)
report = classification_report(y_test, y_pred, output_dict=True)
print(report)
# Create a bar plot for each metric
metrics = ['precision', 'recall', 'f1-score']
labels = ['0', '1', '2', '3', '4']

for metric in metrics:
    values = [report[str(label)][metric] for label in labels]
    # labels = report.keys()
    plt.bar(labels, values)
    plt.xlabel('Class')
    plt.ylabel(metric)
    plt.title(f'{metric.capitalize()} by Class')
    plt.show()



from sklearn.metrics import classification_report
from xgboost import XGBClassifier
#Tell what xgboost is and its working principle

XGB_Data=df_l1

XGB_Data.drop('arm', axis=1)
XGB_Data.drop('Length', axis=1)

# Separate features and target
features = XGB_Data.drop('chromosome', axis=1)
target = XGB_Data['chromosome']

# Stratified sampling
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in splitter.split(features, target):
    X_train, X_test = features.loc[train_index], features.loc[test_index]
    y_train, y_test = target.loc[train_index], target.loc[test_index]

# Train the XGBoost classifier
model = XGBClassifier(random_state=42)
model.fit(features, target)

# Evaluate the model on the test set
print("Accuracy on test set:", model.score(X_test, y_test))

y_pred = model.predict(X_test)

# Calculate the classification report
report = classification_report(y_test, y_pred)

# Print the report
print(report)
report = classification_report(y_test, y_pred, output_dict=True)
print(report)
# Create a bar plot for each metric
metrics = ['precision', 'recall', 'f1-score']
labels = ['0', '1', '2', '3', '4']

for metric in metrics:
    values = [report[str(label)][metric] for label in labels]
    # labels = report.keys()
    plt.bar(labels, values)
    plt.xlabel('Class')
    plt.ylabel(metric)
    plt.title(f'{metric.capitalize()} by Class')
    plt.show()

# prompt: generate neural network model for predicting chromosome

import numpy as np
# Import necessary libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import pandas as pd
# df = pd.concat([annotatedl1_df, annotatedb0_df, annotatedc24_df, annotatedk0_df], axis=0)

XGB_Data=pd.concat([df_b0,df_c24, df_l1, df_k0], axis=0)

print(XGB_Data.head())
XGB_Data.drop('arm', axis=1)
XGB_Data.drop('Length', axis=1)

# Separate features and target
features = XGB_Data.drop('chromosome', axis=1)
target = XGB_Data['chromosome']

# Stratified sampling
splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_index, test_index in splitter.split(features, target):
    X_train, X_test = features.loc[train_index], features.loc[test_index]
    y_train, y_test = target.loc[train_index], target.loc[test_index]

# Define the model
model = keras.Sequential([
    layers.Dense(128, activation='relu', input_shape=(7,)),  # Input layer with 12 features
    layers.Dense(64, activation='relu'),
    layers.Dense(5, activation='softmax')  # Output layer with 5 classes (chromosomes)
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=100)

# Evaluate the model
model.evaluate(X_test, y_test)

# # Predict the chromosome for new data
# new_data = ...  # New data with 12 features
predictions = model.predict(X_test)

# Calculate the classification report
report = classification_report(y_test, y_pred)

# Print the report
print(report)
# Get the predicted chromosome
predicted_chromosome = np.argmax(predictions)

"""## Regression Analysis"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, r2_score

data_regression=df

data_regression.drop('arm', axis=1)
data_regression.drop('chromosome', axis=1)
# Separate features and target
features = data_regression.drop("Length", axis=1)
target = data_regression["Length"]

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)

# Train and evaluate each model
models = {
    "Linear Regression": LinearRegression(),
    "Decision Tree": DecisionTreeRegressor(random_state=24),
    "Random Forest": RandomForestRegressor(random_state=24),
    "Gradient Boosting": GradientBoostingRegressor(random_state=24),
    "Support Vector Regression": SVR(),
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    print(f"Model: {name}")
    # print(f"MSE: {mse}")
    print(f"R-squared_abs: {abs(r2)}")
    # print(f"R-squared: {r2}")
    print()

"""## NN"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.regularizers import L1L2

# Define the model with L1 and L2 regularization
model = Sequential()
model.add(Dense(128, activation="relu", kernel_regularizer=L1L2(l1=0.01, l2=0.01), input_shape=(X_train.shape[1],)))
model.add(Dense(64, activation="relu", kernel_regularizer=L1L2(l1=0.01, l2=0.01)))
model.add(Dense(1))

# Compile the model with Adam optimizer and mean squared error loss
model.compile(loss="mean_squared_error", optimizer="adam")

# Train the model with early stopping
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor="val_loss", patience=10)

history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), callbacks=[early_stopping])

# Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse}")
print(f"R-squared: {r2}")

"""## Analysis of Chromosomes independently"""

# prompt: identify unique df['chromosomes'] and split it into that number of dfs on those unique chromosomes

import pandas as pd

# Get unique chromosome values
unique_chromosomes = df['chromosome'].unique()

# Create a dictionary to store the split DataFrames
chromosome_dfs = {}

# Split the DataFrame based on unique chromosome values
for chromosome in unique_chromosomes:
    chromosome_dfs[chromosome] = df[df['chromosome'] == chromosome]

# Access individual DataFrames using their chromosome value as the key
chromosome_0_df = chromosome_dfs[0]
chromosome_1_df = chromosome_dfs[1]
chromosome_2_df = chromosome_dfs[2]
chromosome_3_df = chromosome_dfs[3]
chromosome_4_df = chromosome_dfs[4]

# chromosome_3_df.tail()

sns.boxplot(x=chromosome_1_df['GC_Content'])

# prompt: find mean length for each chromosome_0_df['Length'] and others

# Calculate the mean length for each chromosome
mean_lengths = {}
for chromosome_df in chromosome_dfs.values():
  chromosome_mean_length = chromosome_df['Length'].mean()
  mean_lengths[chromosome_df['chromosome'].iloc[0]] = chromosome_mean_length

# Print the mean lengths for each chromosome
for chromosome, mean_length in mean_lengths.items():
  print(f"Chromosome {chromosome}: Mean length = {mean_length}")

"""## Snp Analysis

## Data Reading
"""

# prompt: download and load txt data from a url and store it in a dataframe without header

import numpy as np
import pandas as pd
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Bur-0/Marker/Bur-0.SNPs.TAIR8.txt'
df_bur0 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/C24/Marker/C24.SNPs.TAIR8.txt'
df_c24 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Ler-1/Marker/Ler-1.SNPs.TAIR8.txt'
df_ler1 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Kro-0/Marker/Kro-0.SNPs.TAIR8.txt'
df_kro0 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Col-0/Col-0.SNPs.TAIR8.txt'
df_refcol0 = pd.read_table(url, sep='\t', header=None)

df_ler1.tail()

# prompt: assign header to dataframe

df_bur0.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_c24.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_ler1.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_kro0.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_refcol0.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_ler1.tail()

# from pandas import concat as pd_concat
# for df in [df_bur0, df_c24, df_ler1, df_kro0, df_refcol0]:
#     final_df = pd_concat([final_df, df], ignore_index=True)

# # prompt: find unique values from df['samples', Reference base', 'Substitution base'] and map them into numeric features

# unique_samples = final_df['Sample'].unique()
# unique_ref_bases = final_df['Reference base'].unique()
# unique_sub_bases = final_df['Substitution base'].unique()

# sample_to_int = {val: i for i, val in enumerate(unique_samples)}
# ref_base_to_int = {val: i for i, val in enumerate(unique_ref_bases)}
# sub_base_to_int = {val: i for i, val in enumerate(unique_sub_bases)}

# final_df['Sample'] = final_df['Sample'].map(sample_to_int)
# final_df['Reference base'] = final_df['Reference base'].map(ref_base_to_int)
# final_df['Substitution base'] = final_df['Substitution base'].map(sub_base_to_int)

# final_df.head()

# prompt: show correlation coefficient through sns

import matplotlib.pyplot as plt
import seaborn as sns

# Get the correlation matrix
correlation_matrix = final_df.corr()

# Display the correlation matrix using Seaborn
sns.heatmap(correlation_matrix, annot=True)
plt.show()

# prompt: perform pca and show the plot

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Separate features and target
features = final_df.drop('Sample', axis=1)
target = final_df['Sample']

# Perform PCA
pca = PCA(n_components=2)
pca_features = pca.fit_transform(features)

# Create a scatter plot of the PCA features
plt.scatter(pca_features[:, 0], pca_features[:, 1], c=target, cmap='viridis')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA of SNP Data')
plt.show()

# prompt: Here we provide the genome sequences of four Arabidopsis thaliana accessions. The sequences are assembled from Illumina sequence reads only accounting for ~50 to 200x genome coverage. Tilingarrayexpressionanalysis. (A)Effectofprobecorrectiononexpressionestimatesfor7,056genesforwhichhalformoreofallprobeswere removed.Notethatthedistributionisskewedtowardtheestimatesbeinghigheraftercorrection.(B)Expr

!pip install biopython
from Bio import SeqIO
from Bio.Seq import Seq
from Bio.Alphabet import IUPAC

# Read the genome sequences from the FASTA files
genome_sequences = {}
for accession in ["Bur-0", "C24", "Ler-1", "Kro-0"]:
    with open(f"{accession}.fa", "r") as handle:
        genome_sequences[accession] = SeqIO.read(handle, "fasta")

# Extract the first 1000 nucleotides from each genome sequence
for accession, sequence in genome_sequences.items():
    genome_sequences[accession] = sequence[:1000]

# Print the extracted sequences
for accession, sequence in genome_sequences.items():
    print(f"{accession}: {sequence}")



"""# Expression Analysis

## Data Reading
"""

# prompt: download and load txt data from a url and store it in a dataframe without header

import numpy as np
import pandas as pd
from pandas import concat as pd_concat
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Bur-0/Marker/Bur-0.SNPs.TAIR8.txt'
df_bur0 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/C24/Marker/C24.SNPs.TAIR8.txt'
df_c24 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Ler-1/Marker/Ler-1.SNPs.TAIR8.txt'
df_ler1 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Kro-0/Marker/Kro-0.SNPs.TAIR8.txt'
df_kro0 = pd.read_table(url, sep='\t', header=None)
url = 'https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Col-0/Col-0.SNPs.TAIR8.txt'
df_refcol0 = pd.read_table(url, sep='\t', header=None)

# df_ler1.tail()

# prompt: assign header to dataframe

df_bur0.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_c24.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_ler1.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_kro0.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']
df_refcol0.columns = ['Sample', 'Chromosome', 'Position', 'Reference base', 'Substitution base', 'Quality', 'Non-repetitive reads count', 'Concordance','Avg. overlapping reads']


# print(len(df_refcol0))

"""## Bur-0

### Quality Filter
"""

# prompt: calculate and print length of df_bur0 before and after quality filter and plot a bar graph for comparison and give length difference

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
print("Length of df_bur0 before quality filter:", len(df_bur0))

# Filter df_bur0 based on quality
df_bur0_filtered = df_bur0[df_bur0['Quality'] >= 35]

print("Length of df_bur0 after quality filter:", len(df_bur0_filtered))

# Calculate the difference in length
length_difference = len(df_bur0) - len(df_bur0_filtered)

print("Length difference:", length_difference)

# Plot a bar graph for comparison
df_lengths = pd.DataFrame({
    'Sample': ['Before filter', 'After filter'],
    'Length': [len(df_bur0), len(df_bur0_filtered)]
})

sns.barplot(x='Sample', y='Length', data=df_lengths)
plt.show()

import pandas as pd


# Filter data based on quality threshold
quality_threshold = 35  # Set your quality threshold here
df_bur0 = df_bur0[df_bur0['Quality'] >= quality_threshold]


# quality_threshold = 35  # Set your quality threshold here
# df_refcol0 = df_refcol0[df_refcol0['Quality'] >= quality_threshold]
# len(filtered_df)
# filtered_df.tail()

"""### Group by Chromosome and Position"""

# import pandas as pd


# # Group the data by Chromosome and Position
# grouped_df = filtered_df.groupby(['Chromosome', 'Position']).agg({
#     'Sample': list,  # Use 'first' to keep the first value of Sample
#     'Reference base': 'first',
#     'Substitution base': 'first',
#     'Quality': 'mean',  # Take the mean quality for each group
#     'Non-repetitive reads count': 'sum',  # Sum reads
#     'Concordance': 'mean',  # Take the mean concordance for each group
#     'Avg. overlapping reads': 'mean'  # Take the mean alignments
# }).reset_index()

# # Print the organized DataFrame
# print("Organized Data:")

# # Now, you have the data organized by Chromosome and Position,
# # ready for further analysis such as expression analysis.
# print(len(grouped_df))
# grouped_df.tail()

# # prompt: from grouped_df give only those rows with list including Bur-0 and Col-0 under Sample

# filtered_df = grouped_df[grouped_df['Sample'].apply(lambda x: 'Bur-0' in x and 'Col-0' in x)]
# print(filtered_df)

"""### SNP Presence"""

# Assuming your grouped DataFrame is named grouped_df

# Define a function to identify SNPs
def identify_snps(row):
    if row['Reference base'] != row['Substitution base']:
        if row['Sample'] == 'Col-0':
            return 'Col-0'
        elif row['Sample'] == 'Bur-0':
            return 'Bur-0'
        else:
             return 'Not Found'
    else:
        return 'None'

# Apply the function to create a new column
df_bur0['SNP Presence'] = df_bur0.apply(identify_snps, axis=1)
df_refcol0['SNP Presence'] = df_refcol0.apply(identify_snps, axis=1)

# Print the updated DataFrame
print("DataFrame with SNP Presence:")
print(df_bur0.head())
print(df_refcol0.head())

df_bur0['Sample'].unique()

# prompt: only show data frame with None in SNP Presence

# Filter the DataFrame to show only rows with "None" in the "SNP Presence" column
df_none_snp = df_bur0[df_bur0['SNP Presence'] == 'None']

# Print the filtered DataFrame
print(df_none_snp)

"""### Annotation of SNP"""

# prompt: download txt file in colab

!wget https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Ler-1/WGA_Variants/snp.annotation.TAIR8.txt

# prompt: the a txt file to csv dataframe and anything beyond column 6 should merged to column 6 a single entry

import numpy as np
import pandas as pd

# Read the data into a DataFrame
# df = pd.read_csv("snp.annotation.TAIR8.txt", sep="\t", header=None)
df_snp_annotation = pd.read_csv("snp_b0.annotation.TAIR8.txt", sep="\t", header=None, usecols=range(6))
df_snp_annotation.columns = ['Sample', 'Chromosome', 'Position', 'Reference allele', 'New allele', 'Annotation']

df_snp_annotation.head(20)
len(df_snp_annotation)
# # Select the first four columns
# df = df.iloc[:, :4]

# # Merge remaining columns into the fourth column
# df.iloc[:, 4:] = df.iloc[:, 4:].apply(lambda x: ",".join(x.astype(str)), axis=1)

# # Print the DataFrame
# print(df.head())

import pandas as pd

# Merge grouped_df with snp_annotation_df based on Chromosome and Position
annotatedb0_df = pd.merge(df_bur0, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
annotatedc0_df = pd.merge(df_refcol0, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
# Print the annotated DataFrame
# print("Annotated Data:")
# print(annotatedb0_df.head())
# print(annotatedc0_df.head())

# prompt: from annotatedb0_df give bar plot for different annotations in the data and give values for different annotations

import matplotlib.pyplot as plt
# Get the annotation counts
annotation_counts = annotatedb0_df['Annotation'].value_counts()

# Create a bar plot of the annotation counts
plt.bar(annotation_counts.index, annotation_counts.values)
plt.xlabel('Annotation')
plt.ylabel('Count')
plt.title('Annotation Counts in Bur-0')
plt.show()

# Print the annotation counts
print(annotation_counts)

# # prompt: download annotated_df
# from google.colab import files
# annotated_df.to_csv('annotated_df.csv', index=False)
# files.download('annotated_df.csv')

# # prompt: Remove rows with NaN annotation

# del_annotated_df = annotated_df[annotated_df['Annotation'].isna()]

# print(len(del_annotated_df))
# del_annotated_df.head()
# # len(del_annotated_df)

"""### Expression Analysis and Statistical Tests"""

import pandas as pd
from scipy.stats import ttest_ind
from scipy.stats import mannwhitneyu
# Concatenate the two dataframes
annotated_df = pd.concat([annotatedb0_df, annotatedc0_df])

# Group the data by gene (Annotation)
grouped_by_gene = annotated_df.groupby('Annotation')
# Group the data by gene
# grouped_by_gene_b0 = annotatedb0_df.groupby('Annotation')
# grouped_by_gene_c0 = annotatedb0_df.groupby('Annotation')
# Dictionary to store expression levels for each gene
expression_levels = {}

# Calculate expression levels for each gene in Col-0 and Bur-0
for gene, data in grouped_by_gene:

    # print('gene: ',gene)
    # print('data: ', data)
    # Count the number of reads supporting each allele for Col-0 and Bur-0

    # Separate the data for Bur-0 and Col-0
    data_bur0 = data[data['Sample_x'] == 'Bur-0']
    data_col0 = data[data['Sample_x'] == 'Col-0']

    # print('data_col0: ', data_col0)

    # Count the number of reads supporting each allele for Col-0 and Bur-0
    col0_reads = data_col0['Non-repetitive reads count']
    bur0_reads = data_bur0['Non-repetitive reads count']

    # print('col0_reads: ', col0_reads)

    # Normalize counts
    # col0_normalized = col0_reads / col0_reads.sum()  # Example of simple normalization (dividing by total counts)
    # bur0_normalized = bur0_reads / bur0_reads.sum()
    # print('col0_normalized: ', col0_normalized)

    # # Log transformation
    # col0_expression = col0_normalized.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    # bur0_expression = bur0_normalized.apply(lambda x: np.log2(x + 1))

    # Log transformation
    col0_expression = col0_reads.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    bur0_expression = bur0_reads.apply(lambda x: np.log2(x + 1))

    # Store expression levels in the dictionary
    expression_levels[gene] = {'Bur-0': bur0_reads, 'Col-0': col0_reads}

"""T_test"""

# print(expression_levels.items())
# Perform statistical tests for each gene
for gene, expr in expression_levels.items():
    col0_expr = expr['Col-0']
    bur0_expr = expr['Bur-0']

    # Perform t-test
    t_stat,_ = ttest_ind(col0_expr, bur0_expr)

    # Print or store results
    print(f"Gene: {gene}, t-statistic: {abs(t_stat)}")

"""1. CDS:
The t-statistic here is 4.73.
Think of this as a measure of how different the expression levels are between Col-0 and Bur-0 for genes classified as CDS.
The bigger the t-statistic, the more different the expressions are. So, 4.73 is quite big!
It suggests that there's a noticeable difference in expression levels between Col-0 and Bur-0 for genes classified as CDS.
2. five_prime_UTR:
The t-statistic is 3.13.
It's not as big as for CDS, but still quite noticeable.
This means there's a reasonable difference in expression levels between Col-0 and Bur-0 for genes classified as five_prime_UTR.
3. intergenic:
Here, the t-statistic is 8.84!
This is quite large, showing a big difference in expression levels between Col-0 and Bur-0 for genes classified as intergenic.
It's like saying, "Wow, there's a huge difference!"
4. intronic:
The t-statistic is 7.09.
It's similar to intergenic, showing a significant difference in expression levels between Col-0 and Bur-0 for genes classified as intronic.
5. three_prime_UTR:
Lastly, the t-statistic is 2.75.
It's smaller compared to the others, but still noticeable.

This suggests there's a difference in expression levels between Col-0 and Bur-0 for genes classified as three_prime_UTR, but it's not as big as for the other gene types.
So, these t-statistics help us understand how much the expression levels differ between Col-0 and Bur-0 for each type of gene. Larger values mean bigger differences.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Data
genes = ['CDS', 'five_prime_UTR', 'intergenic', 'intronic', 'three_prime_UTR']
t_statistics = [4.73, 3.13, 8.84, 7.09, 2.75]  # Or p-values if you prefer

# Bar Plot
plt.figure(figsize=(10, 6))
plt.bar(genes, t_statistics, color='skyblue')
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistics')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#  Heatmap
heatmap_data = pd.DataFrame({'Gene Type': genes, 'T-Statistic': t_statistics})
heatmap_data = heatmap_data.set_index('Gene Type')
sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt=".2f")
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistic')
plt.tight_layout()
plt.show()

"""Mann-Whitney U Test"""

# print(expression_levels.items())
# Perform statistical tests for each gene
for gene, expr in expression_levels.items():
      col0_expr = expr['Col-0']
      bur0_expr = expr['Bur-0']

      # Perform U-test
      _, p_value = mannwhitneyu(col0_expr, bur0_expr)

      # Print or store results
      print(f"Gene: {gene}, p-value: {abs(p_value)}")
      # Assuming col0_expr and bur0_expr are arrays of expression levels for Col-0 and Bur-0

"""Results:
1. CDS:
The Mann-Whitney U test found a very small p-value, which is like saying there's a very small chance that this result happened by random chance.
It's so small, in fact, that it looks like a very tiny number: 0.00001238.
This means there's strong evidence to suggest that there's a real difference in expression levels between Col-0 and Bur-0 for genes classified as CDS.
2. five_prime_UTR:
The p-value here is also very small, but not as tiny as for CDS. It's 0.00048.
This still means there's a very low chance that this result is random.
It suggests that there's a real difference in expression levels between Col-0 and Bur-0 for genes classified as five_prime_UTR.
3. intergenic:
The p-value is extremely small, even smaller than for CDS. It's a really tiny number: 0.0000000000000000000423!
This shows a super strong evidence that there's a difference in expression levels between Col-0 and Bur-0 for genes classified as intergenic. It's almost certain there's a real difference.
4. intronic:
Here, the p-value is still very small, but not as incredibly tiny as for intergenic. It's 0.00000000004236.
This suggests a very strong likelihood that there's a difference in expression levels between Col-0 and Bur-0 for genes classified as intronic.
5. three_prime_UTR:
Lastly, the p-value is 0.0147, which is small but not super tiny like the others.
This suggests there's still a reasonable chance this result could happen by random chance.
It indicates there might be a difference in expression levels between Col-0 and Bur-0 for genes classified as three_prime_UTR, but we're less sure about it compared to the other gene types.

So, in short, these p-values tell us how confident we are that there's a real difference in expression levels between Col-0 and Bur-0 for each type of gene. Lower p-values mean we're more confident in the difference.

These t-statistics indicate the magnitude of difference in gene expression between Col-0 and Bur-0 for different gene regions. Let's interpret these results:

1. **CDS (Coding Sequences)**:
   - **T-Statistic (t = 4.73)**: This suggests a significant difference in gene expression between Col-0 and Bur-0 in coding sequences. It means that the average expression level of genes in coding sequences is 4.73 standard deviations higher in one accession compared to the other.

2. **Five Prime UTR**:
   - **T-Statistic (t = 3.13)**: There is a moderate difference in gene expression between Col-0 and Bur-0 in the 5' untranslated regions (UTRs). Genes in this region show a higher average expression in one accession compared to the other, but the difference is not as large as in coding sequences.

3. **Intergenic Regions**:
   - **T-Statistic (t = 8.84)**: The expression of genes in intergenic regions shows a very significant difference between Col-0 and Bur-0. This large t-statistic indicates that there is a substantial difference in gene expression in these non-coding regions between the two accessions.

4. **Intronic Regions**:
   - **T-Statistic (t = 7.09)**: Genes located within intronic regions also show a significant difference in expression between Col-0 and Bur-0. This suggests that the regulation of gene expression in intronic regions differs between the two accessions.

5. **Three Prime UTR**:
   - **T-Statistic (t = 2.75)**: There is a smaller but still noticeable difference in gene expression between Col-0 and Bur-0 in the 3' untranslated regions (UTRs). Genes in this region show a slightly higher or lower average expression in one accession compared to the other.

Biological interpretation:
- The significant difference in expression observed in coding sequences (CDS) suggests that there may be genetic differences between Col-0 and Bur-0 affecting protein-coding genes.
- Differences in expression in non-coding regions (such as UTRs and intergenic regions) could indicate variations in regulatory elements or non-coding RNAs.
- These findings may highlight genes and pathways that play crucial roles in the phenotypic differences between Col-0 and Bur-0, such as growth, development, stress response, or disease resistance.
- Further investigation into the specific genes showing significant differences could provide insights into the molecular mechanisms underlying these differences and their impact on plant physiology and adaptation.

## Ler-1
"""

import pandas as pd

##Quality Filter
# prompt: calculate and print length of df_bur0 before and after quality filter and plot a bar graph for comparison and give length difference

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
print("Length of df_bur0 before quality filter:", len(df_ler1))

# Filter df_bur0 based on quality
df_ler1_filtered = df_ler1[df_ler1['Quality'] >= 35]

print("Length of df_bur0 after quality filter:", len(df_ler1_filtered))

# Calculate the difference in length
length_difference = len(df_ler1) - len(df_ler1_filtered)

print("Length difference:", length_difference)

# Plot a bar graph for comparison
df_lengths = pd.DataFrame({
    'Sample': ['Before filter', 'After filter'],
    'Length': [len(df_ler1), len(df_ler1_filtered)]
})

sns.barplot(x='Sample', y='Length', data=df_lengths)
plt.show()
# Filter data based on quality threshold
quality_threshold = 30  # Set your quality threshold here
df_ler1 = df_ler1[df_ler1['Quality'] >= quality_threshold]


quality_threshold = 30  # Set your quality threshold here
df_refcol0 = df_refcol0[df_refcol0['Quality'] >= quality_threshold]

##SNP PRESENCE

# Define a function to identify SNPs
def identify_snps(row):
    if row['Reference base'] != row['Substitution base']:
        if row['Sample'] == 'Col-0':
            return 'Col-0'
        elif row['Sample'] == 'Ler-1':
            return 'Ler-1'
        # else:
        #     return 'Both'
    else:
        return 'None'

# Apply the function to create a new column
df_ler1['SNP Presence'] = df_ler1.apply(identify_snps, axis=1)
df_refcol0['SNP Presence'] = df_refcol0.apply(identify_snps, axis=1)
# prompt: only show data frame with None in SNP Presence

# Filter the DataFrame to show only rows with "None" in the "SNP Presence" column
df_none_snp = df_ler1[df_ler1['SNP Presence'] == 'None']

# Print the filtered DataFrame
print("None snp:",df_none_snp)

# # Print the updated DataFrame
# print("DataFrame with SNP Presence:")
# print(df_ler1.head())
# print(df_refcol0.head())

##ANNOTATION

import numpy as np
import pandas as pd

# Read the data into a DataFrame
# df = pd.read_csv("snp.annotation.TAIR8.txt", sep="\t", header=None)
df_snp_annotation = pd.read_csv("snp_l1.annotation.TAIR8.txt", sep="\t", header=None, usecols=range(6))
df_snp_annotation.columns = ['Sample', 'Chromosome', 'Position', 'Reference allele', 'New allele', 'Annotation']

# df_snp_annotation.head(20)
# len(df_snp_annotation)



import pandas as pd

# Merge grouped_df with snp_annotation_df based on Chromosome and Position
annotatedl1_df = pd.merge(df_ler1, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
annotatedc0_df = pd.merge(df_refcol0, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
# # Print the annotated DataFrame
# print("Annotated Data:")
# print(annotatedl1_df.head())
# print(annotatedc0_df.head())
# prompt: from annotatedb0_df give bar plot for different annotations in the data and give values for different annotations

import matplotlib.pyplot as plt
# Get the annotation counts
annotation_counts = annotatedl1_df['Annotation'].value_counts()

# Create a bar plot of the annotation counts
plt.bar(annotation_counts.index, annotation_counts.values)
plt.xlabel('Annotation')
plt.ylabel('Count')
plt.title('Annotation Counts in Ler-1')
plt.show()

# Print the annotation counts
print(annotation_counts)
##EXPRESSION ANALYSIS

import pandas as pd
from scipy.stats import ttest_ind
from scipy.stats import mannwhitneyu
# Concatenate the two dataframes
annotated_df = pd.concat([annotatedl1_df, annotatedc0_df])

# Group the data by gene (Annotation)
grouped_by_gene = annotated_df.groupby('Annotation')
# Group the data by gene
# grouped_by_gene_b0 = annotatedb0_df.groupby('Annotation')
# grouped_by_gene_c0 = annotatedb0_df.groupby('Annotation')
# Dictionary to store expression levels for each gene
expression_levels = {}

# Calculate expression levels for each gene in Col-0 and Bur-0
for gene, data in grouped_by_gene:

    # print('gene: ',gene)
    # print('data: ', data)
    # Count the number of reads supporting each allele for Col-0 and Bur-0

    # Separate the data for Bur-0 and Col-0
    data_ler1 = data[data['Sample_x'] == 'Ler-1']
    data_col0 = data[data['Sample_x'] == 'Col-0']

    # print('data_col0: ', data_col0)

    # Count the number of reads supporting each allele for Col-0 and Bur-0
    col0_reads = data_col0['Non-repetitive reads count']
    ler1_reads = data_ler1['Non-repetitive reads count']

    # print('col0_reads: ', col0_reads)

    # Normalize counts
    # col0_normalized = col0_reads / col0_reads.sum()  # Example of simple normalization (dividing by total counts)
    # bur0_normalized = bur0_reads / bur0_reads.sum()
    # print('col0_normalized: ', col0_normalized)

    # # Log transformation
    # col0_expression = col0_normalized.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    # bur0_expression = bur0_normalized.apply(lambda x: np.log2(x + 1))

    # Log transformation
    col0_expression = col0_reads.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    ler1_expression = ler1_reads.apply(lambda x: np.log2(x + 1))

    # Store expression levels in the dictionary
    expression_levels[gene] = {'Ler-1': ler1_reads, 'Col-0': col0_reads}


# print(expression_levels.items())
# Perform statistical tests for each gene
g_l1=[]
t_s_l1=[]

for gene, expr in expression_levels.items():
    col0_expr = expr['Col-0']
    ler1_expr = expr['Ler-1']

    # Perform t-test
    t_stat,_ = ttest_ind(col0_expr, ler1_expr)

    # Print or store results
    g_l1.append(gene)
    t_s_l1.append(abs(t_stat))
    print(f"Gene: {gene}, t-statistic: {abs(t_stat)}")

import matplotlib.pyplot as plt
import seaborn as sns

# Data
# genes = ['CDS', 'five_prime_UTR', 'intergenic', 'intronic', 'three_prime_UTR']
# # t_statistics = [4.73, 3.13, 8.84, 7.09, 2.75]  # Or p-values if you prefer

# Bar Plot
plt.figure(figsize=(10, 6))
plt.bar(g_l1, t_s_l1, color='skyblue')
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistics')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#  Heatmap
heatmap_data = pd.DataFrame({'Gene Type': g_l1, 'T-Statistic': t_s_l1})
heatmap_data = heatmap_data.set_index('Gene Type')
sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt=".2f")
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistic')
plt.tight_layout()
plt.show()

"""## C24"""

import pandas as pd

##Quality Filter
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
print("Length of df_bur0 before quality filter:", len(df_c24))

# Filter df_bur0 based on quality
df_c24_filtered = df_c24[df_c24['Quality'] >= 35]

print("Length of df_bur0 after quality filter:", len(df_c24_filtered))

# Calculate the difference in length
length_difference = len(df_c24) - len(df_c24_filtered)

print("Length difference:", length_difference)

# Plot a bar graph for comparison
df_lengths = pd.DataFrame({
    'Sample': ['Before filter', 'After filter'],
    'Length': [len(df_c24), len(df_c24_filtered)]
})

sns.barplot(x='Sample', y='Length', data=df_lengths)
plt.show()
# Filter data based on quality threshold
quality_threshold = 30  # Set your quality threshold here
df_c24 = df_c24[df_c24['Quality'] >= quality_threshold]


quality_threshold = 30  # Set your quality threshold here
df_refcol0 = df_refcol0[df_refcol0['Quality'] >= quality_threshold]

##SNP PRESENCE

# Define a function to identify SNPs
def identify_snps(row):
    if row['Reference base'] != row['Substitution base']:
        if row['Sample'] == 'Col-0':
            return 'Col-0'
        elif row['Sample'] == 'C24':
            return 'C24'
        # else:
        #     return 'Both'
    else:
        return 'None'

# Apply the function to create a new column
df_c24['SNP Presence'] = df_c24.apply(identify_snps, axis=1)
df_refcol0['SNP Presence'] = df_refcol0.apply(identify_snps, axis=1)

# Filter the DataFrame to show only rows with "None" in the "SNP Presence" column
df_none_snp = df_c24[df_c24['SNP Presence'] == 'None']

# Print the filtered DataFrame
print("None snp:",df_none_snp)
# # Print the updated DataFrame
# print("DataFrame with SNP Presence:")
# print(df_c24.head())
# print(df_refcol0.head())

##ANNOTATION

import numpy as np
import pandas as pd

# Read the data into a DataFrame
# df = pd.read_csv("snp.annotation.TAIR8.txt", sep="\t", header=None)
df_snp_annotation = pd.read_csv("snp_c24.annotation.TAIR8.txt", sep="\t", header=None, usecols=range(6))
df_snp_annotation.columns = ['Sample', 'Chromosome', 'Position', 'Reference allele', 'New allele', 'Annotation']

# df_snp_annotation.head(20)
# len(df_snp_annotation)



import pandas as pd

# Merge grouped_df with snp_annotation_df based on Chromosome and Position
annotatedc24_df = pd.merge(df_c24, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
annotatedc0_df = pd.merge(df_refcol0, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
# # Print the annotated DataFrame
# print("Annotated Data:")
# print(annotatedc24_df.head())
# print(annotatedc0_df.head())
import matplotlib.pyplot as plt
# Get the annotation counts
annotation_counts = annotatedc24_df['Annotation'].value_counts()

# Create a bar plot of the annotation counts
plt.bar(annotation_counts.index, annotation_counts.values)
plt.xlabel('Annotation')
plt.ylabel('Count')
plt.title('Annotation Counts in C24')
plt.show()

# Print the annotation counts
print(annotation_counts)
##EXPRESSION ANALYSIS

import pandas as pd
from scipy.stats import ttest_ind
from scipy.stats import mannwhitneyu
# Concatenate the two dataframes
annotated_df = pd.concat([annotatedc24_df, annotatedc0_df])

# Group the data by gene (Annotation)
grouped_by_gene = annotated_df.groupby('Annotation')
# Group the data by gene
# grouped_by_gene_b0 = annotatedb0_df.groupby('Annotation')
# grouped_by_gene_c0 = annotatedb0_df.groupby('Annotation')
# Dictionary to store expression levels for each gene
expression_levels = {}

# Calculate expression levels for each gene in Col-0 and Bur-0
for gene, data in grouped_by_gene:

    # print('gene: ',gene)
    # print('data: ', data)
    # Count the number of reads supporting each allele for Col-0 and Bur-0

    # Separate the data for Bur-0 and Col-0
    data_c24 = data[data['Sample_x'] == 'C24']
    data_col0 = data[data['Sample_x'] == 'Col-0']

    # print('data_col0: ', data_col0)

    # Count the number of reads supporting each allele for Col-0 and Bur-0
    col0_reads = data_col0['Non-repetitive reads count']
    c24_reads = data_c24['Non-repetitive reads count']

    # print('col0_reads: ', col0_reads)

    # Normalize counts
    # col0_normalized = col0_reads / col0_reads.sum()  # Example of simple normalization (dividing by total counts)
    # bur0_normalized = bur0_reads / bur0_reads.sum()
    # print('col0_normalized: ', col0_normalized)

    # # Log transformation
    # col0_expression = col0_normalized.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    # bur0_expression = bur0_normalized.apply(lambda x: np.log2(x + 1))

    # Log transformation
    col0_expression = col0_reads.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    c24_expression = c24_reads.apply(lambda x: np.log2(x + 1))

    # Store expression levels in the dictionary
    expression_levels[gene] = {'C24': c24_reads, 'Col-0': col0_reads}


# print(expression_levels.items())
# Perform statistical tests for each gene
g_c24=[]
t_s_c24=[]

for gene, expr in expression_levels.items():
    col0_expr = expr['Col-0']
    c24_expr = expr['C24']

    # Perform t-test
    t_stat,_ = ttest_ind(col0_expr, c24_expr)

    # Print or store results
    g_c24.append(gene)
    t_s_c24.append(abs(t_stat))
    print(f"Gene: {gene}, t-statistic: {abs(t_stat)}")

import matplotlib.pyplot as plt
import seaborn as sns

# Data
# genes = ['CDS', 'five_prime_UTR', 'intergenic', 'intronic', 'three_prime_UTR']
# # t_statistics = [4.73, 3.13, 8.84, 7.09, 2.75]  # Or p-values if you prefer

# Bar Plot
plt.figure(figsize=(10, 6))
plt.bar(g_c24, t_s_c24, color='skyblue')
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistics')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#  Heatmap
heatmap_data = pd.DataFrame({'Gene Type': g_c24, 'T-Statistic': t_s_c24})
heatmap_data = heatmap_data.set_index('Gene Type')
sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt=".2f")
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistic')
plt.tight_layout()
plt.show()

"""## Kro-0"""

import pandas as pd

##Quality Filter
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
print("Length of df_bur0 before quality filter:", len(df_kro0))

# Filter df_bur0 based on quality
df_kro0_filtered = df_kro0[df_kro0['Quality'] >= 35]

print("Length of df_bur0 after quality filter:", len(df_kro0_filtered))

# Calculate the difference in length
length_difference = len(df_kro0) - len(df_kro0_filtered)

print("Length difference:", length_difference)

# Plot a bar graph for comparison
df_lengths = pd.DataFrame({
    'Sample': ['Before filter', 'After filter'],
    'Length': [len(df_kro0), len(df_kro0_filtered)]
})

sns.barplot(x='Sample', y='Length', data=df_lengths)
plt.show()
# Filter data based on quality threshold
quality_threshold = 30  # Set your quality threshold here
df_kro0 = df_kro0[df_kro0['Quality'] >= quality_threshold]


quality_threshold = 30  # Set your quality threshold here
df_refcol0 = df_refcol0[df_refcol0['Quality'] >= quality_threshold]

##SNP PRESENCE

# Define a function to identify SNPs
def identify_snps(row):
    if row['Reference base'] != row['Substitution base']:
        if row['Sample'] == 'Col-0':
            return 'Col-0'
        elif row['Sample'] == 'Kro-0':
            return 'Kro-0'
        # else:
        #     return 'Both'
    else:
        return 'None'

# Apply the function to create a new column
df_kro0['SNP Presence'] = df_kro0.apply(identify_snps, axis=1)
df_refcol0['SNP Presence'] = df_refcol0.apply(identify_snps, axis=1)

# Filter the DataFrame to show only rows with "None" in the "SNP Presence" column
df_none_snp = df_kro0[df_kro0['SNP Presence'] == 'None']

# Print the filtered DataFrame
print("None snp:",df_none_snp)
# # Print the updated DataFrame
# print("DataFrame with SNP Presence:")
# print(df_kro0.head())
# print(df_refcol0.head())

##ANNOTATION

import numpy as np
import pandas as pd

# Read the data into a DataFrame
# df = pd.read_csv("snp.annotation.TAIR8.txt", sep="\t", header=None)
df_snp_annotation = pd.read_csv("snp_k0.annotation.TAIR8.txt", sep="\t", header=None, usecols=range(6))
df_snp_annotation.columns = ['Sample', 'Chromosome', 'Position', 'Reference allele', 'New allele', 'Annotation']

# df_snp_annotation.head(20)
# len(df_snp_annotation)



import pandas as pd

# Merge grouped_df with snp_annotation_df based on Chromosome and Position
annotatedk0_df = pd.merge(df_kro0, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
annotatedc0_df = pd.merge(df_refcol0, df_snp_annotation,
                        left_on=['Chromosome', 'Position'],
                        right_on=['Chromosome', 'Position'],
                        how='left')
# # Print the annotated DataFrame
# print("Annotated Data:")
# print(annotatedk0_df.head())
# print(annotatedc0_df.head())
import matplotlib.pyplot as plt
# Get the annotation counts
annotation_counts = annotatedk0_df['Annotation'].value_counts()

# Create a bar plot of the annotation counts
plt.bar(annotation_counts.index, annotation_counts.values)
plt.xlabel('Annotation')
plt.ylabel('Count')
plt.title('Annotation Counts in kro-0')
plt.show()

# Print the annotation counts
print(annotation_counts)
##EXPRESSION ANALYSIS

import pandas as pd
from scipy.stats import ttest_ind
from scipy.stats import mannwhitneyu
# Concatenate the two dataframes
annotated_df = pd.concat([annotatedk0_df, annotatedc0_df])

# Group the data by gene (Annotation)
grouped_by_gene = annotated_df.groupby('Annotation')
# Group the data by gene
# grouped_by_gene_b0 = annotatedb0_df.groupby('Annotation')
# grouped_by_gene_c0 = annotatedb0_df.groupby('Annotation')
# Dictionary to store expression levels for each gene
expression_levels = {}

# Calculate expression levels for each gene in Col-0 and Bur-0
for gene, data in grouped_by_gene:

    # print('gene: ',gene)
    # print('data: ', data)
    # Count the number of reads supporting each allele for Col-0 and Bur-0

    # Separate the data for Bur-0 and Col-0
    data_kro0 = data[data['Sample_x'] == 'Kro-0']
    data_col0 = data[data['Sample_x'] == 'Col-0']

    # print('data_col0: ', data_col0)

    # Count the number of reads supporting each allele for Col-0 and Bur-0
    col0_reads = data_col0['Non-repetitive reads count']
    kro0_reads = data_kro0['Non-repetitive reads count']

    # print('col0_reads: ', col0_reads)

    # Normalize counts
    # col0_normalized = col0_reads / col0_reads.sum()  # Example of simple normalization (dividing by total counts)
    # bur0_normalized = bur0_reads / bur0_reads.sum()
    # print('col0_normalized: ', col0_normalized)

    # # Log transformation
    # col0_expression = col0_normalized.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    # bur0_expression = bur0_normalized.apply(lambda x: np.log2(x + 1))

    # Log transformation
    col0_expression = col0_reads.apply(lambda x: np.log2(x + 1))  # Add pseudocount to avoid log(0)
    kro0_expression = kro0_reads.apply(lambda x: np.log2(x + 1))

    # Store expression levels in the dictionary
    expression_levels[gene] = {'Kro-0': kro0_reads, 'Col-0': col0_reads}


# print(expression_levels.items())
# Perform statistical tests for each gene
g_k0=[]
t_s_k0=[]

for gene, expr in expression_levels.items():
    col0_expr = expr['Col-0']
    kro0_expr = expr['Kro-0']

    # Perform t-test
    t_stat,_ = ttest_ind(col0_expr, kro0_expr)

    # Print or store results
    g_k0.append(gene)
    t_s_k0.append(abs(t_stat))
    print(f"Gene: {gene}, t-statistic: {abs(t_stat)}")

import matplotlib.pyplot as plt
import seaborn as sns

# Data
# genes = ['CDS', 'five_prime_UTR', 'intergenic', 'intronic', 'three_prime_UTR']
# # t_statistics = [4.73, 3.13, 8.84, 7.09, 2.75]  # Or p-values if you prefer

# Bar Plot
plt.figure(figsize=(10, 6))
plt.bar(g_k0, t_s_k0, color='skyblue')
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistics')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#  Heatmap
heatmap_data = pd.DataFrame({'Gene Type': g_k0, 'T-Statistic': t_s_k0})
heatmap_data = heatmap_data.set_index('Gene Type')
sns.heatmap(heatmap_data, cmap='coolwarm', annot=True, fmt=".2f")
plt.title('T-Statistics by Gene Type')
plt.xlabel('Gene Type')
plt.ylabel('T-Statistic')
plt.tight_layout()
plt.show()

"""# Gene Prediction

## preprocess
"""

# prompt: concat dataframes

import pandas as pd
df = pd.concat([annotatedl1_df, annotatedb0_df, annotatedc24_df, annotatedk0_df], axis=0)

# prompt: drop multiple columns

df.drop(['SNP Presence', 'Sample_y', 'New allele', 'Reference allele'], axis=1, inplace=True)
df.head()

# prompt: map following categorical columns to numerical by finding their uniques values. 'Sample_x', 'Reference base', 'Substitution base', 'Annotation'

sample_uniques = df['Sample_x'].unique()
sample_map = {val: i for i, val in enumerate(sample_uniques)}
df['Sample_x'] = df['Sample_x'].map(sample_map)

ref_uniques = df['Reference base'].unique()
ref_map = {val: i for i, val in enumerate(ref_uniques)}
df['Reference base'] = df['Reference base'].map(ref_map)

sub_uniques = df['Substitution base'].unique()
sub_map = {val: i for i, val in enumerate(sub_uniques)}
df['Substitution base'] = df['Substitution base'].map(sub_map)

annotation_uniques = df['Annotation'].unique()
annotation_map = {val: i for i, val in enumerate(annotation_uniques)}
df['Annotation'] = df['Annotation'].map(annotation_map)

df.head()

# prompt: use anova techniques to find best features. Also, show correlation matrix using sns

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

# # Load the data
# df = pd.read_csv("your_data.csv")

# Separate features and target
X = df.drop("Annotation", axis=1)
y = df["Annotation"]

# Feature selection using ANOVA
selector = SelectKBest(f_classif, k=5)
X_selected = selector.fit_transform(X, y)

# Get the ANOVA F-values
f_values = selector.scores_

# Create a DataFrame with features and their F-values
anova_df = pd.DataFrame({"Feature": X.columns, "F-value": f_values})

# Sort the DataFrame by F-value
anova_df = anova_df.sort_values(by="F-value", ascending=False)

# Print the top 10 features with the highest F-values
print("Top 5 features with the highest F-values:")
print(anova_df.head(10))

# Calculate the correlation matrix
correlation_matrix = df.corr()

# Create a heatmap of the correlation matrix
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.show()

"""## XGB"""

# prompt: apply XGBoost. target is Annotation

from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.metrics import classification_report

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, stratify=y, random_state=42)

# Stratified sampling
# splitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
# for train_index, test_index in splitter.split(X, y):
#     X_train, X_test = X.loc[train_index], X.loc[test_index]
#     y_train, y_test = y.loc[train_index], y.loc[test_index]

# Create an XGBClassifier model
model = XGBClassifier()

# Train the model
model.fit(X_train, y_train)

# Predict the labels for the test set
y_pred = model.predict(X_test)

# Print the classification report
print(classification_report(y_test, y_pred))

# prompt: for classification report give a bar plot

import matplotlib.pyplot as plt

# Get the classification report as a dictionary
report = classification_report(y_test, y_pred, output_dict=True)

# Create a bar plot for each metric
metrics = ['precision', 'recall', 'f1-score']
labels = ['0', '1', '2', '3', '4', '5']

for metric in metrics:
    values = [report[str(label)][metric] for label in labels]
    # labels = report.keys()
    plt.bar(labels, values)
    plt.xlabel('Class')
    plt.ylabel(metric)
    plt.title(f'{metric.capitalize()} by Class')
    plt.show()

"""## NN"""

# prompt: give distribution percentage of classes

class_counts = df['Annotation'].value_counts()
total_samples = len(df)
for class_name, count in class_counts.items():
    percentage = count / total_samples * 100
    print(f"Class: {class_name}, Percentage: {percentage:.2f}%")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from tensorflow import keras




# Separate features and target
X = df.drop("Annotation", axis=1)
y = df["Annotation"]

# Encode the target labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
y = to_categorical(y)

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# # Handle class imbalance using SMOTE
# smote = SMOTE(random_state=42)
# X_train, y_train = smote.fit_resample(X_train, y_train)


# Create a sequential model
model = Sequential()

# Add the first hidden layer with 64 units and ReLU activation
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))

# Add a dropout layer to prevent overfitting
model.add(Dropout(0.2))

# Add the second hidden layer with 32 units and ReLU activation
model.add(Dense(32, activation='relu'))

# Add another dropout layer
model.add(Dropout(0.2))

# Add the output layer with 5 units and softmax activation
model.add(Dense(6, activation='softmax'))

# Compile the model with Adam optimizer, categorical cross-entropy loss, and accuracy metric
model.compile(optimizer = keras.optimizers.SGD(learning_rate=0.01), loss='categorical_crossentropy', metrics=['accuracy'])
# Calculate class weights manually
from sklearn.utils.class_weight import compute_class_weight

# Get the unique classes and their counts
unique_classes, class_counts = np.unique(y_train, return_counts=True)

# Calculate the class weights
total_samples = len(y_train)
class_weights = total_samples / (len(unique_classes) * class_counts)

# Create a dictionary mapping classes to their respective weights
class_weight_dict = dict(zip(unique_classes, class_weights))

# Train the model with class weights
model.fit(X_train, y_train, epochs=10, batch_size=32, class_weight=class_weight_dict)

# Create a dictionary mapping classes to their respective weights
class_weight_dict = dict(enumerate(class_weights))

# Train the model with class weights
model.fit(X_train, y_train, epochs=10, batch_size=32, class_weight=class_weight_dict)
# Train the model
# model.fit(X_train, y_train, epochs=5, batch_size=16)

# Evaluate the model on the test set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
y_test = np.argmax(y_test, axis=1)  # Convert one-hot encoded back to numerical labels
print(classification_report(y_test, y_pred))

# prompt: for classification report give a bar plot

import matplotlib.pyplot as plt

# Get the classification report as a dictionary
report = classification_report(y_test, y_pred, output_dict=True)

# Create a bar plot for each metric
metrics = ['precision', 'recall', 'f1-score']
labels = ['0', '1', '2', '3', '4', '5']

for metric in metrics:
    values = [report[str(label)][metric] for label in labels]
    # labels = report.keys()
    plt.bar(labels, values)
    plt.xlabel('Class')
    plt.ylabel(metric)
    plt.title(f'{metric.capitalize()} by Class')
    plt.show()

# prompt: only give percentage distribution for y_test
print("Y_test")
class_counts = {}
for label in y_test:
    # print(label)
    if label not in class_counts:
        class_counts[label] = 0
    class_counts[label] += 1

total_samples = len(y_test)
for class_name, count in class_counts.items():
    percentage = count / total_samples * 100
    print(f"Class: {class_name}, Percentage: {percentage:.2f}%")

# prompt: give unique classes for y_train without using np

unique_classes = list(set(y_train))

"""## Random Forest"""

# prompt: now apply random forest
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Separate features and target
X = df.drop("Annotation", axis=1)
y = df["Annotation"]

# Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# prompt: now apply random forest, with option to set number of trees

# Define the Random Forest classifier with 100 trees
rf_model = RandomForestClassifier(n_estimators=20)

# Train the model
rf_model.fit(X_train, y_train)

# Predict the labels for the test set
y_pred_rf = rf_model.predict(X_test)

# # Print the classification report
# print(classification_report(y_test, y_pred_rf))

# prompt: for classification report give a bar plot

import matplotlib.pyplot as plt

# Get the classification report as a dictionary
report = classification_report(y_test, y_pred_rf, output_dict=True)
print(report)
# Create a bar plot for each metric
metrics = ['precision', 'recall', 'f1-score']
labels = ['0', '1', '2', '3', '4', '5']

for metric in metrics:
    values = [report[str(label)][metric] for label in labels]
    # labels = report.keys()
    plt.bar(labels, values)
    plt.xlabel('Class')
    plt.ylabel(metric)
    plt.title(f'{metric.capitalize()} by Class')
    plt.show()

from sklearn.metrics import classification_report

# Print the classification report
print(classification_report(y_test, y_pred_rf))

# prompt: download txt file in colab

!wget https://1001genomes.org/data/MPI/MPISchneeberger2011/releases/current/Ler-1/WGA_Variants/snp.annotation.TAIR8.txt